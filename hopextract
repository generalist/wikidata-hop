#!/bin/bash

# script for extracting various metadata from HoP HTML pages and preparing for Wikidata

# nb this needs them all in a handy directory at eg 1386-1421/member/xyz.html

# uses pup - https://github.com/ericchiang/pup

# refresh with 
# curl "https://raw.githubusercontent.com/generalist/wikidata-hop/master/hopextract" > hopextract

rm working/*
rm educated

# produce a raw list of the educated section

for i in `ls */member/*.html` ; do 
echo -e $i "\t" `cat $i | pup --color | grep -A 2 "educ$" | tail -1` >> educated ;
echo -e $i "\t" `cat $i | pup --color | grep -A 2 "educ.$" | tail -1` >> educated ;
done
# these are the two forms used



# pull out the schools (just schools for now as proof of concept)
# drop them into single text files for now so that they can be manually reviewed

grep "Eton" educated > working/school-eton # Q192088
grep "Harrow" educated > working/school-harrow # Q1247373
grep "Rugby" educated > working/school-rugby # Q1143281
grep "Westminster" educated > working/school-westminster # Q1341516
grep "Winchester" educated > working/school-winchester # Q1059517

# now the colleges/universities

grep "Christ Church, Oxf" educated > working/coll-cc-ox # Q745967
# lots more Oxbridge to do, but these are very inconsistent - lots of variation

grep "Edinburgh Univ" educated > working/uni-edi # Q160302
grep "Glasgow Univ" educated > working/uni-gla # Q192775
grep "Aberdeen Univ" educated > working/uni-aber # Q270532
grep "St. Andrews Univ" educated > working/uni-stan # Q216273
grep "Trinity, Dublin" educated > working/uni-tcd # Q258464

# and the Inns

grep "M. Temple" educated > working/inn-m-temple # Q925942
grep "I. Temple" educated > working/inn-i-temple # Q1233784
grep "L. Inn" educated > working/inn-l # Q69482
grep "L.Inn" educated >> working/inn-l # Q69482 
grep "G. Inn" educated > working/inn-g # Q157412
grep "Kingâ€™s Inn" educated > working/inn-king # Q2069586
grep "King's Inn" educated >> working/inn-king # Q2069586

# now we have a lot of extracted people, we can build an index and upload away

# constrained as ONLY ETON for now

curl "http://tools.wmflabs.org/wikidata-todo/beacon.php?prop=1614&source=0" | sed 's/||/\t/g' > working/beacon

for i in `cat working/school-eton | cut -f 1 | sed 's/.html//g'` ; do grep $i working/beacon >> working/list-eton ; done

# list of all Q-ids and corresponding slugs for etonians

# now find only one for each - the use of tail is to ensure it's the later volume as standard (consistent with other uses)

for i in `cat working/list-eton | cut -f 1 | sort | uniq` ; do grep $i working/list-eton | tail -n 1 >> working/trimmedlist-eton ; done

# now build the QuickStatements line
# for now, this only uses a simple referenceURL cite

rm quickstatements

for i in `cat working/trimmedlist-eton | cut -f 1` ; do echo -e $i"\t"P69"\t"Q192088"\t"S854"\t""\"http://www.historyofparliamentonline.org/volume/"`grep $i working/trimmedlist-eton | cut -f 2`\" >> quickstatements ; done


# QuickStatements is pretty smart - if the claim exists but has no reference, it will simply add the reference
# if it exists with a different reference, it will add another one
# this means we could even cite it to every HOP entry, but that would be overkill, let's leave in the one-only rule :-)
