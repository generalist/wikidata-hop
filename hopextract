#!/bin/bash

# script for extracting various metadata from HoP HTML pages and preparing for Wikidata

# nb this needs them all in a handy directory at eg 1386-1421/member/xyz.html

# uses pup - https://github.com/ericchiang/pup

# refresh with 
# curl "https://raw.githubusercontent.com/generalist/wikidata-hop/master/hopextract" > hopextract

# tidy up from previous runs

rm working/*
rm quickstatements

# import some useful stuff

curl "http://tools.wmflabs.org/wikidata-todo/beacon.php?prop=1614&source=0" | sed 's/||/\t/g' | grep -v \# > working/beacon

cut -f 1 working/beacon | sort | uniq > working/onwiki-qids
cut -f 2 working/beacon | sort > working/onwiki-slugs

# pull in the list of matched institutions

curl "https://raw.githubusercontent.com/generalist/wikidata-hop/master/institutions.tsv" > working/institutions.tsv

# for each member that exists onwiki, extract the 'educated' section

for i in `cat working/onwiki-slugs` ; do 
echo -e $i "\t" `cat $i.html | pup --color | grep -A 2 "educ$" | tail -1` >> working/educated ;
echo -e $i "\t" `cat $i.html | pup --color | grep -A 2 "educ.$" | tail -1` >> working/educated ;
done

# these are the two forms used
# it will produce a lot of blank superfluous sections, but eh, not a major problem
# they'll be ignored by the following searches

# here is a quick-and-dirty overview to highlight the most important ones to import

cut -f 2 working/educated | sed 's/;/\t/g' | sed 's/\t/\n/g' | sed 's/ [0-9]/\t/g' | sed 's/ c./\t/g' | sed 's/^ //g' | sed 's/^\. //g' | cut -f 1 | sed 's/ $//g' | sed 's/\.$//g'| sed 's/,$//g' | sort | uniq -c | sort -rn > working/rawnumbers
for i in `cat working/institutions.tsv | cut -f 1` ; do grep "`grep $i working/institutions.tsv | cut -f 2`" working/rawnumbers >> working/scratch ; done
cat working/scratch | sort | uniq | sort -rn > working/rawmatched
grep -vf working/rawmatched working/rawnumbers > working/rawunmatched

# in other words... 
# rawnumbers - overall crudely-deduplicated report
# rawmatched - number that will be matched by one of the dedicated regexps
# rawunmatched - number that is not matched by a regexp

# an interesting problem to consider - how to avoid, eg, Brasenose matching "privately by... fellow of Brasenose"
# a good move might be to omit any fragments containing "privately"

exit


##### proof of concept below

grep "Eton" working/educated > working/school-eton # Q192088

# now we have a lot of extracted people, we can build an index and upload away

for i in `cat working/school-eton | cut -f 1` ; do grep $i working/beacon >> working/list-eton ; done

# list of all Q-ids and corresponding slugs for etonians

# now find only one for each - the use of tail is to ensure it's the later volume as standard (consistent with other uses)

for i in `cat working/list-eton | cut -f 1 | sort | uniq` ; do grep $i working/list-eton | tail -n 1 >> working/trimmedlist-eton ; done

# now build the QuickStatements line
# for now, this only uses a simple referenceURL cite

for i in `cat working/trimmedlist-eton | cut -f 1` ; do echo -e $i"\t"P69"\t"Q192088"\t"S854"\t""\"http://www.historyofparliamentonline.org/volume/"`grep $i working/trimmedlist-eton | cut -f 2`\" >> quickstatements ; done

# QuickStatements is pretty smart - if the claim exists but has no reference, it will simply add the reference
# if it exists with a different reference, it will add another one
# this means we could even cite it to every HOP entry, but that would be overkill, let's leave in the one-only rule :-)
