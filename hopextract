#!/bin/bash

# script for extracting various metadata from HoP HTML pages and preparing for Wikidata

# nb this needs them all in a handy directory at eg 1386-1421/member/xyz.html

# uses pup - https://github.com/ericchiang/pup

# refresh with 
# curl "https://raw.githubusercontent.com/generalist/wikidata-hop/master/hopextract" > hopextract

# tidy up from previous runs

rm working/*
rm quickstatements

# import some useful stuff

curl "http://tools.wmflabs.org/wikidata-todo/beacon.php?prop=1614&source=0" | sed 's/||/\t/g' | grep -v \# > working/beacon

cut -f 1 working/beacon | sort | uniq > working/onwiki-qids
cut -f 2 working/beacon | sort > working/onwiki-slugs

# for each member that exists onwiki, extract the 'educated' section

for i in `cat working/onwiki-slugs` ; do 
echo -e $i "\t" `cat $i.html | pup --color | grep -A 2 "educ$" | tail -1` >> working/educated ;
echo -e $i "\t" `cat $i.html | pup --color | grep -A 2 "educ.$" | tail -1` >> working/educated ;
done

# these are the two forms used
# it will produce a lot of blank superfluous sections, but eh, not a major problem
# they'll be ignored by the following searches

# here is a quick-and-dirty overview to highlight the most important ones to import

cut -f 2 working/educated | sed 's/;/\t/g' | sed 's/\t/\n/g' | sed 's/ [0-9]/\t/g' | sed 's/ c./\t/g' | sed 's/^ //g' | sed 's/^\. //g' | cut -f 1 | sed 's/ $//g' | sed 's/\.$//g'| sed 's/,$//g' | sort | uniq -c | sort -rn > working/rawnumbers


# pull out the schools (just schools for now as proof of concept)
# drop them into single text files for now so that they can be manually reviewed

grep "Eton" working/educated > working/school-eton # Q192088
grep "Harrow" working/educated > working/school-harrow # Q1247373
grep "Rugby" working/educated > working/school-rugby # Q1143281
grep "Westminster" working/educated > working/school-westminster # Q1341516
grep "Winchester" working/educated > working/school-winchester # Q1059517

# now the colleges/universities

grep "Christ Church, Oxf" working/educated > working/coll-cc-ox # Q745967
# lots more Oxbridge to do, but these are very inconsistent - lots of variation

grep "Edinburgh Univ" working/educated > working/uni-edi # Q160302
grep "Glasgow Univ" working/educated > working/uni-gla # Q192775
grep "Aberdeen Univ" working/educated > working/uni-aber # Q270532
grep "St. Andrews Univ" working/educated > working/uni-stan # Q216273
grep "Trinity, Dublin" working/educated > working/uni-tcd # Q258464

# and the Inns

grep "M. Temple" working/educated > working/inn-m-temple # Q925942
grep "I. Temple" working/educated > working/inn-i-temple # Q1233784
grep "L. Inn" working/educated > working/inn-l # Q69482
grep "L.Inn" working/educated >> working/inn-l # Q69482 
grep "G. Inn" working/educated > working/inn-g # Q157412
grep "Kingâ€™s Inn" working/educated > working/inn-king # Q2069586
grep "King's Inn" working/educated >> working/inn-king # Q2069586

# now we have a lot of extracted people, we can build an index and upload away

# constrained as ONLY ETON for now


for i in `cat working/school-eton | cut -f 1` ; do grep $i working/beacon >> working/list-eton ; done

# list of all Q-ids and corresponding slugs for etonians

# now find only one for each - the use of tail is to ensure it's the later volume as standard (consistent with other uses)

for i in `cat working/list-eton | cut -f 1 | sort | uniq` ; do grep $i working/list-eton | tail -n 1 >> working/trimmedlist-eton ; done

# now build the QuickStatements line
# for now, this only uses a simple referenceURL cite


for i in `cat working/trimmedlist-eton | cut -f 1` ; do echo -e $i"\t"P69"\t"Q192088"\t"S854"\t""\"http://www.historyofparliamentonline.org/volume/"`grep $i working/trimmedlist-eton | cut -f 2`\" >> quickstatements ; done


# QuickStatements is pretty smart - if the claim exists but has no reference, it will simply add the reference
# if it exists with a different reference, it will add another one
# this means we could even cite it to every HOP entry, but that would be overkill, let's leave in the one-only rule :-)

