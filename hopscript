#!/bin/bash

# this should be run from within the /scripts directory, and uses

cd ~/scripts
mkdir -p working
rm working/*

# the script for scraping the HOP site would go here
# however, it does not need to be run every time
# when it does, hop/memberlinks contains a file of extracted link fragments in the form
###### 1386-1421/member/abbot-john
# grep -v XREF hop/memberlinks > working/templinks
# will remove all the crossreferences
# for now, let's use a locally stored copy

curl "https://raw.githubusercontent.com/generalist/wikidata-hop/links/links" > working/templinks

# this cleans it up to be unique

curl "http://tools.wmflabs.org/wikidata-todo/beacon.php?prop=1614&source=0" | sed 's/||/\t/g' > working/beacon

curl "http://tools.wmflabs.org/wikidata-todo/beacon.php?prop=1614&source=1415" | sed 's/|/\t/g' | grep -v "#"  > working/beacon-odnb
curl "http://tools.wmflabs.org/wikidata-todo/beacon.php?prop=1614&source=18" | sed 's/|/\t/g' | grep -v "#"  > working/beacon-image
curl "http://tools.wmflabs.org/wikidata-todo/beacon.php?prop=1614&source=1816" | sed 's/|/\t/g' | grep -v "#"  > working/beacon-npg
curl "http://tools.wmflabs.org/wikidata-todo/beacon.php?prop=1614&source=1802" | sed 's/|/\t/g' | grep -v "#"  > working/beacon-emlo
curl "http://tools.wmflabs.org/wikidata-todo/beacon.php?prop=1614&source=214" | sed 's/|/\t/g' | grep -v "#"  > working/beacon-viaf
curl "http://tools.wmflabs.org/wikidata-todo/beacon.php?prop=1614&source=213" | sed 's/|/\t/g' | grep -v "#"  > working/beacon-isni
curl "http://tools.wmflabs.org/wikidata-todo/beacon.php?prop=1614&source=2015" | sed 's/|/\t/g' | grep -v "#"  > working/beacon-hansard

# get all the properties we're interested in from the BEACON tool

cut -f 2 working/beacon | grep -v "#" > working/liveids

grep -Fxvf working/liveids working/templinks > working/unmatchedlinks
grep -Fxvf working/liveids working/templinks | sed 's/%28/(/g' | sed 's/%29/)/g' | sed 's/%E2%80%99/â€™/g' > working/links1
sort -t'/' -V -k3 working/links1 | uniq -u > working/links

# find all lines in templinks not in liveids, then clean them up if needed, and sort by member name (third col after /)

# now assemble everything for the export file

cut -f 1 working/beacon | sort | uniq | grep -v "#" > working/allids

echo \# \export of Wikidata to History of Parliament matches >> working/report.tsv
echo \# note that a single person may have 1-3 seperate HoP entries, depending on the length and timing of their career >> working/report.tsv
echo \# provided on `date` >> working/report.tsv
echo \# updated daily at http:\/\/www.generalist.org.uk\/wikidata >> working/report.tsv
echo \# queries to Andrew Gray, andrew@generalist.org.uk >> working/report.tsv
echo \# >> working/report.tsv
echo \# `cat working/liveids | wc -l` entries on Wikidata - `cut -f 1 working/beacon | sort | uniq | wc -l` people - `cut -f 1 working/beacon-odnb | sort | uniq | wc -l` ODNB - `cut -f 1 working/beacon-npg | sort | uniq | wc -l` NPG - `cut -f 1 working/beacon-emlo | sort | uniq | wc -l` EMLO - `cut -f 1 working/beacon-viaf | sort | uniq | wc -l` VIAF - `cut -f 1 working/beacon-isni | sort | uniq | wc -l` ISNI - `cut -f 1 working/beacon-hansard | sort | uniq | wc -l` Hansard. >> working/report.tsv
echo \# >> working/report.tsv
echo \# `grep 1386-1421 working/links | wc -l` of 3246 still unmatched from 1386-1421 >> working/report.tsv
echo \# `grep 1509-1558 working/links | wc -l` of 2266 still unmatched from 1509-1558 >> working/report.tsv
echo \# `grep 1558-1603 working/links | wc -l` of 2670 still unmatched from 1558-1603 >> working/report.tsv
echo \# `grep 1604-1629 working/links | wc -l` of 1755 still unmatched from 1604-1629 >> working/report.tsv
echo \# `grep 1660-1690 working/links | wc -l` of 2354 still unmatched from 1660-1690 >> working/report.tsv
echo \# `grep 1715-1754 working/links | wc -l` of 2041 still unmatched from 1715-1754 >> working/report.tsv
echo \# `grep 1754-1790 working/links | wc -l` of 1966 still unmatched from 1754-1790 >> working/report.tsv
echo \# `grep 1790-1820 working/links | wc -l` of 2533 still unmatched from 1790-1820 >> working/report.tsv
echo \# `grep 1820-1832 working/links | wc -l` of 1367 still unmatched from 1820-1832 >> working/report.tsv
echo \# Overall, `cat working/links | wc -l` of 22181 still unmatched >> working/report.tsv
echo \# >> working/report.tsv
echo \# `cut -b 18- working/links | uniq -d | wc -l` still unmatched with identical titles in different periods >> working/report.tsv
echo \# >> working/report.tsv
echo \# >> working/report.tsv

echo -e Wikidata ID"\t"HoP matches"\t"OBIN"\t"National Portrait Gallery ID"\t"EMLO"\t"VIAF"\t"ISNI"\t"Hansard"\t"HoP-1"\t"HoP-2"\t"HoP-3 >> working/report.tsv
for i in `cat working/allids` ;
do grep -w $i working/beacon >> working/$i
echo -e $i"\t"`grep -wc $i working/beacon`"\t"`grep -w $i working/beacon-odnb | cut -f 1 | uniq`"\t"`grep -w $i working/beacon-npg | cut -f 1 | uniq`"\t"`grep -w $i working/beacon-emlo | cut -f 1 | uniq`"\t"`grep -w $i working/beacon-viaf | cut -f 1 | uniq`"\t"`grep -w $i working/beacon-isni | cut -f 1 | uniq`"\t"`grep -w $i working/beacon-hansard | cut -f 1 | uniq`"\t"`cut -f2 working/$i | sed -n 1p`"\t"`cut -f2 working/$i | sed -n 2p`"\t"`cut -f2 working/$i | sed -n 3p` >> working/report.tsv ;
done

# "| uniq" added to stop multiple entries for eg ODNB when there are two article matches

# this last bit takes all the HoP entries, dumps them to one file per q-number,
# and then reads off lines 1,2,3 (and failing that, blank tabs) - this means we
# can handle entities with multiple values

cp working/report.tsv ~/public_html/wikidata/hop-report.tsv
cp working/links ~/public_html/wikidata/hop-links.txt

# dump all of this into the public_html directory so as to be readable

rm working/*
