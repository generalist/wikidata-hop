#!/bin/bash

# script for extracting various metadata from HoP HTML pages and preparing for Wikidata

# nb this needs them all in a handy directory at eg 1386-1421/member/xyz.html

# the old one was quite clunky - essentially ran once for each matching item
# version 2 runs backwards and takes a known item, then pulls out what matches it can

# uses pup - https://github.com/ericchiang/pup

# refresh with 
# curl "https://raw.githubusercontent.com/generalist/wikidata-hop/master/hopextract-educated" > hopextract-educated

rm working/*
rm quickstatements

# import some useful stuff

curl "http://tools.wmflabs.org/wikidata-todo/beacon.php?prop=1614&source=0" | sed 's/||/\t/g' | grep -v \# > working/beacon

# comment out to have a cheaty beacon
# curl "http://tools.wmflabs.org/wikidata-todo/beacon.php?prop=1614&source=0" | sed 's/||/\t/g' | grep -v \# | grep 123 > working/beacon
# gives us a nice sample set of 36 entries including six doubles

cut -f 1 working/beacon | sort | uniq > working/onwiki-qids
cut -f 2 working/beacon | sort > working/onwiki-slugs

# pull in the list of matched institution strings

# curl "https://raw.githubusercontent.com/generalist/wikidata-hop/master/institutions.tsv" > working/institutions.tsv

# for each member that exists onwiki, extract the 'educated' section

for i in `cat working/onwiki-slugs` ; do 
echo -e $i "\t" `cat $i.html | pup --color | grep -A 2 "educ$" | tail -1` >> working/educated ;
echo -e $i "\t" `cat $i.html | pup --color | grep -A 2 "educ.$" | tail -1` >> working/educated ;
done

cat working/educated | sed 's/;/\t/g' | sed 's/\t/\n/g' | sed 's/ [0-9]/\t/g' | sed 's/ c./\t/g' | sed 's/^ //g' | sed 's/^\. //g' | cut -f 1 | sed 's/ $//g' | sed 's/\.$//g'| sed 's/,$//g' | sort | uniq -c | sort -rn > working/rawnumbers
for i in `cat working/institutions.tsv | cut -f 1` ; do grep "`grep $i working/institutions.tsv | cut -f 2`" working/rawnumbers >> working/scratch ; done
cat working/scratch | sort | uniq | sort -rn > working/rawmatched
grep -vf working/rawmatched working/rawnumbers > working/rawunmatched

# in other words... 
# rawnumbers - overall crudely-deduplicated report
# rawmatched - number that will be matched by one of the dedicated regexps
# rawunmatched - number that is not matched by a regexp

# okay, now let's do this for each person

rm working/personfragments

for i in `cat working/onwiki-qids` ; do

  rm working/pages ;
  grep $i working/beacon | cut -f 2 > working/pages ;

# find all the pages matched to this person

    for j in `cat working/pages` ; do
    rm working/educated     # clear it just to be safe
    rm working/fragments    # clear it just to be safe
    cat $j.html | pup --color | grep -A 2 "educ$" | tail -1 | sed 's/; /\n/g' | sed 's/\t//g' > working/educated ; 
    cat $j.html | pup --color | grep -A 2 "educ.$" | tail -1 | sed 's/; /\n/g' | sed 's/\t//g' >> working/educated ;
    cat working/educated | sed 's/; /\n/g' | sed 's/^ //g' | sed 's/^ //g' | sed 's/^\. //g' | sed 's/ $//g' | sed 's/\.$//g'| sed 's/,$//g' | sed 's/\t//g' > working/fragments ;

# at this point, we have working/fragments containing one line per "bit" of education from a single HoP entry
# we now need to combine it into a single file per person, noting which one from which entry
# this bit creates a variable k, set as the number of lines in working/fragments (and reset to 0 each time for safety)
# if zero, it doesn't do anything (no blank lines!)
# otherwise it cuts line k, then drops it into a nicely formatted new file
# then it takes one off k and does it again
# the test run successfully got two lines from the 1660 entry for Q2897123, and three from the 1690 entry

    k=0 # reset it for safety
    k="`cat working/fragments | wc -l`" # count lines to run the next section

          while [ $k -ne 0 ] ; do
          awk -v l=$k 'NR==l' working/fragments > working/kmfrag
          echo -e $i"\t"$j"\t"`cat working/kmfrag` >> working/personfragments
          ((k--))
          echo $k
          done
    done

 done

# this takes each page, extracts the educated bit, breaks it into fragments, then saves these against the slug and QID
# note Q5537123 in our test - he has two different entries, yay!

# now, what do we want to do with this? we want to parse it, yes we do.


########
#
# STILL TO DO
#
########

# all the parsing, obviously
# - including a solution to the "privately" problem
# a way to deal with brackets, which currently cause problems for the beacon parser
# date parsing wow this will be amazing

# parsing notes -
# omit anything involving the word "privately" unless perhaps it matches a grand tour etc?
# omit any lines *starting* "fellow" eg Q922508
# omit anything with ? perhaps possibly poss.

